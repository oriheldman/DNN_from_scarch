import numpy as np
from keras.datasets import mnist

def initialize_parameters(layer_dims):
    """
    The function initializes the DNN's weights and bias, given an architecture
    :param layer_dims: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened
    input, layer L is the output softmax)
    :return:a dictionary containing the initialized W and b parameters of each layer
    """
    layers_params = {}
    for i, d in enumerate(layer_dims):
        if i == 0:
            continue
        layer_i_w = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.1
        layer_i_b = np.zeros([layer_dims[i], 1])
        layers_params[f'W{i}'] = layer_i_w
        layers_params[f'b{i}'] = layer_i_b
    return layers_params


def linear_forward(A, W, b):
    """
    Implement the linear part of a layer's forward propagation.
    :param A: the activations of the previous layer
    :param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    :param b: the bias vector of the current layer (of shape [size of current layer, 1])
    :return (z, linear_cache): the linear component of the activation function, cache for later compute
    """
    z = np.dot(W, A) + b
    linear_cache = {'A': A, 'W': W, 'b': b}
    return z, linear_cache


def softmax(Z):
    """
    preforms softmax on Z
    :param Z: the linear component of the activation function
    :return: the activations of the layer and activation cache Z
    """
    softmax_z = np.exp(Z) / np.sum(np.exp(Z), axis=0)
    return softmax_z, {"Z": Z}


def relu(Z):
    """
    preforms relu on Z
    :param Z: the linear component of the activation function
    :return: the activations of the layer and activation cache
    """
    relu_z = np.maximum(0, Z)
    return relu_z, {"Z": Z}


def linear_activation_forward(A_prev, W, B, activation):
    """
    the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations of the previous layer
    :param W: the weights matrix of the current layer
    :param B: the bias vector of the current layer
    :param activation: the activation function to be used (a string, either “softmax” or “relu”)
    :return: the activations of the current layer AND a joint dictionary containing linear_cache and activation_cache
    """
    Z, linear_cache = linear_forward(A_prev, W, B)
    if activation == "softmax":
        A, act_cache = softmax(Z)
    else:
        A, act_cache = relu(Z)

    linear_cache.update(act_cache)
    return A, linear_cache


def L_model_forward(X, parameters, use_batchnorm):
    """
    the forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation
    :param X: the data, numpy array of shape (input size, number of examples)
    :param parameters: the initialized W and b parameters of each layer
    :param use_batchnorm: a boolean flag used to determine whether to apply batch normalization after the activation
    :return: the last post-activation value AND a list of all the cache objects generated by the linear_forward
    """
    caches = []
    n_layers = int(max(parameters)[-1])
    layers_input = X
    for i in range(n_layers):

        if parameters['MODEL_PARAMS']['Dropout'] and parameters['MODEL_PARAMS']['Train_mode']:
            layers_input = apply_dropout(layers_input, parameters['MODEL_PARAMS']['Dropout_prob'])

        if i == n_layers - 1:  # Last layer case - apply softmax
            act = 'softmax'
        else:
            act = 'relu'

        W = parameters[f'W{i + 1}']
        b = parameters[f'b{i + 1}']
        A, act_cache = linear_activation_forward(layers_input, W, b, act)

        if use_batchnorm and act == 'relu':
            if parameters['MODEL_PARAMS']['Train_mode']:
                A, mean_val, std_val = apply_batchnorm(A)
                save_batchnorm_values(parameters, i, mean_val, std_val)
            else:
                A = apply_batchnorm_testmode(A, parameters['MODEL_PARAMS'][f'BN_L{i + 1}']['mean'],
                                             parameters['MODEL_PARAMS'][f'BN_L{i + 1}']['std'])
        caches.append(act_cache)
        layers_input = A
    return A, caches


def compute_cost(AL, Y):
    """
    categorical cross-entropy loss
    :param AL: probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
    :param Y: the labels vector (i.e. the ground truth)
    :return: the cross-entropy cost
    """
    m = Y.shape[1]
    cost = - (1 / m) * np.sum(Y * np.log(AL))
    return cost


def Linear_backward(dZ, cache):
    """
    the linear part of the backward propagation process for a single layer
    :param dZ: the gradient of the cost with respect to the linear output of the current layer (layer l)
    :param cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
    :return (dA_prev, dW, db): Gradients of the cost with respect to A_prev, W and b
    """
    A_prev, W, b = cache['A'], cache['W'], cache['b']
    bsize = A_prev.shape[1]

    dW = np.dot(dZ, A_prev.T) / bsize
    db = dZ.sum(axis=1, keepdims=True) / bsize
    dA_prev = np.dot(W.T, dZ)

    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation):
    """
    the backward propagation for the LINEAR->ACTIVATION layer.
    The function first computes dZ and then applies the linear_backward function
    :param dA: post activation gradient of the current layer
    :param cache: contains both the linear cache and the activations cache
    :param activation: type of activation fuction
    :return (dA_prev, dW, db): Gradients of the cost with respect to A_prev, W and b
    """
    if activation == 'relu':
        dZ = relu_backward(dA, cache)
    else:
        dZ = softmax_backward(dA, cache)
    dA_prev, dW, db = Linear_backward(dZ, cache)
    return dA_prev, dW, db


def relu_backward(dA, activation_cache):
    """
    backward propagation for a ReLU unit
    :param dA: the post-activation gradient
    :param activation_cache: Z
    :return: gradient of the cost with respect to Z
    """
    Z = activation_cache['Z']
    Z[Z < 0] = 0
    Z[Z > 0] = 1
    dZ = dA * Z
    return dZ


def softmax_backward(dA, activation_cache):
    """
    backward propagation for a softmax unit
    :param dA: the post-activation gradient
    :param activation_cache: Z
    :return: gradient of the cost with respect to Z
    """
    return dA - activation_cache['Y']


def L_model_backward(AL, Y, caches):
    """
    backward propagation process for the entire network
    :param AL: the probabilities vector, the output of the forward propagation
    :param Y:the true labels vector
    :param caches: list of caches containing for each layer: a) the linear cache; b) the activation
    :return: a dictionary with the gradients
    """
    grad_results = {}
    dA = AL
    for i, cache in reversed(list(enumerate(caches))):
        if i == len(caches) - 1:
            activation = 'softmax'
            cache['Y'] = Y
        else:
            activation = 'relu'
        dA, dW, db = linear_activation_backward(dA, cache, activation)
        grad_results[f'dA{i + 1}'] = dA
        grad_results[f'dW{i + 1}'] = dW
        grad_results[f'db{i + 1}'] = db
    return grad_results


def Update_parameters(parameters, grads, learning_rate):
    """
    updates NN parameters using gradient descent
    :param parameters: a python dictionary containing the DNN architecture’s parameters
    :param grads:a python dictionary containing the
    :param learning_rate: the learning rate
    :return: the updated values of the parameters
    """
    for parameter in parameters:
        if 'W' in parameter or 'b' in parameter:
            parameters[parameter] = parameters[parameter] - (learning_rate * grads[f'd{parameter}'])
    return parameters


def Predict(X, Y, parameters):
    """
    The function receives an input data and the true labels and calculates the accuracy of the trained neural network
     on the data
    :param X: the input data, a numpy array of shape (height*width, number_of_examples)
    :param Y: labels of the data, a vector of shape (num_of_classes, number of examples)
    :param parameters: a python dictionary containing the DNN architecture’s parameters
    :return: the accuracy measure of the neural net on the provided data
    """
    parameters['MODEL_PARAMS']['Train_mode'] = False

    # X, _, _ = standarize_data(X, parameters['MODEL_PARAMS']['train_pixels_mean'], parameters['MODEL_PARAMS']['train_pixels_std'])
    X = norm_data(X)

    preds, _ = L_model_forward(X, parameters, use_batchnorm=parameters['MODEL_PARAMS']['Batchnorm'])
    preds = np.argmax(preds, axis=0)
    Y = np.argmax(Y, axis=0)

    parameters['MODEL_PARAMS']['Train_mode'] = True
    return np.mean(preds == Y) * 100


def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size=512):
    """
    An L-layer neural network. All layers but the last have the ReLU activation function, and the final
     layer applies the softmax activation function. The size of the output layer should be equal to the number of
      labels in the data.
    :param X: the input data, a numpy array of shape (height*width , number_of_examples)
    :param Y: the “real” labels of the data
    :param layers_dims: a list containing the dimensions of each layer (including the input)
    :param learning_rate: learning rate for training
    :param num_iterations: the number of max iterations the networks trains for
    :param batch_size: the number of examples in a single training batch
    :return (parameters, cost): the parameters learnt, and the values of the cost function
    """
    np.random.seed(42)

    parameters = initialize_parameters(layers_dims)
    parameters['MODEL_PARAMS'] = {}

    X_train, X_val, y_train, y_val = split_train_test(X, Y, test_size=0.2, random_seed=42)

    X_train = norm_data(X_train)
    X_val_normed = norm_data(X_val)

    parameters['MODEL_PARAMS']['Batchnorm'] = False
    parameters['MODEL_PARAMS']['Dropout'] = False
    parameters['MODEL_PARAMS']['Dropout_prob'] = 0.1

    if parameters['MODEL_PARAMS']['Batchnorm']:
        init_batchnorm_accumulators(parameters)

    parameters['MODEL_PARAMS']['Train_mode'] = True

    X_train_batches, y_train_batches = split_to_batches(X_train, y_train, batch_size, drop_last=True)
    n_iter_in_epoch = len(X_train_batches)
    train_costs = []
    # val_costs = []

    max_iterations_no_improvement = 500
    n_iterations_for_computing_cost = 5
    n_no_improvement_iterations = 0
    min_val_cost = np.inf

    for iter in range(num_iterations):

        if n_no_improvement_iterations == max_iterations_no_improvement:
            break

        X = X_train_batches[iter % n_iter_in_epoch]
        Y = y_train_batches[iter % n_iter_in_epoch]

        AL, caches = L_model_forward(X, parameters, parameters['MODEL_PARAMS']['Batchnorm'])
        cost = compute_cost(AL, Y)

        # calc validation loss for stopping criteria
        if (iter + 1) % n_iterations_for_computing_cost == 0:
            val_preds, _ = L_model_forward(X_val_normed, parameters, parameters['MODEL_PARAMS']['Batchnorm'])
            val_cost = compute_cost(val_preds, y_val)
            if val_cost < min_val_cost:
                min_val_cost = val_cost
                n_no_improvement_iterations = 0
            else:
                n_no_improvement_iterations += n_iterations_for_computing_cost

        # save iteration results
        if (iter + 1) % 100 == 0:
            train_costs.append(cost)
            # epoch = round(iter / n_iter_in_epoch)
            # print(f'Epoch: {epoch} iter: {iter} train_cost: {cost} val_cost: {val_cost}')
            # val_costs.append(val_cost)

        grads = L_model_backward(AL, Y, caches)
        parameters = Update_parameters(parameters, grads, learning_rate)

    # val_acc = Predict(X_val, y_val, parameters)
    return parameters, train_costs


def apply_batchnorm(A):
    """
    performs batchnorm on the received activation values of a given layer
    :param A: the activation values of a given layer
    :return: the normalized activation values, based on the formula learned in class
    """
    eps = 0.000001
    mean_val = np.mean(A, axis=1, keepdims=True)
    std_val = np.std(A, axis=1, keepdims=True) + eps
    A_centered = A - mean_val
    A_normed = A_centered / std_val
    return A_normed, mean_val, std_val


# # # # # # Auxiliary Functions  # # # # # #


def apply_batchnorm_testmode(A, mean_values, std_values):
    """
    apply batchnorm on layer output in testmode
    :param A: prev layer activation values
    :param mean_values: accumulated mean values from train
    :param std_values: accumulated std values from train
    :return: A - after batch normed
    """
    eps = 0.000001
    A_centered = A - np.mean(mean_values, axis=0)
    A_normed = A_centered / (np.mean(std_values, axis=0) + eps)
    return A_normed


def init_batchnorm_accumulators(parameters):
    """
    This function init empty lists for batchnorm accumulators, from this lists we compute the mean for
    applying batchnorm on test samples.
    :param parameters: parameters dict of the model
    :return:
    """
    n_layers = int(max(parameters)[-1])
    max_size = 100
    for i in range(n_layers):
        parameters['MODEL_PARAMS'][f'BN_L{i + 1}'] = {}
        parameters['MODEL_PARAMS'][f'BN_L{i + 1}']['mean'] = [0] * max_size
        parameters['MODEL_PARAMS'][f'BN_L{i + 1}']['std'] = [0] * max_size


def save_batchnorm_values(parameters, i, mean_val, std_val):
    """
    Add the mean values to parameters dict (accumulators)
    :param parameters: NN parameters dict
    :param i: layer index
    :param mean_val: layer's batch mean of meav values
    :param std_val:  layer's batch mean of std values
    :return: None
    """
    layer_mean_list = parameters['MODEL_PARAMS'][f'BN_L{i + 1}']['mean']
    layer_std_list = parameters['MODEL_PARAMS'][f'BN_L{i + 1}']['std']

    layer_mean_list.pop(0)
    layer_std_list.pop(0)

    layer_mean_list.append(mean_val)
    layer_std_list.append(std_val)


def apply_dropout(input, dropout_prob):
    """
    Apply dropout on a layer
    :param input:
    :param dropout_prob: probability to mask out neuron
    :return: input masked with dropout
    """
    mask = np.random.rand(input.shape[0], input.shape[1]) > dropout_prob
    input = input * mask
    input = input * (1 / (1 - dropout_prob))
    return input


def split_to_batches(x, y, batch_size, drop_last):
    """
    Splits the data into batches of size batch_size.
    :param x: The input data
    :param y: The labels
    :param batch_size: The size of a single batch
    :param drop_last: if true, the last batch will be removed if its size is not equal to batch_size
    :return:
    """
    x_batches = []
    y_batches = []
    for i in range(0, x.shape[1], batch_size):
        x_batches.append(x[:, i:i + batch_size])
        y_batches.append(y[:, i:i + batch_size])

    if drop_last and len(x_batches[-1]) != batch_size:
        x_batches = x_batches[:-1]
        y_batches = y_batches[:-1]
    return x_batches, y_batches


def indice_to_onehot(y):
    """
    converts an np array of indices to onehot vector array
    :param y: the indices to be converted
    :return: np.array of onehot vectors
    """
    y_onehot = np.zeros((y.size, y.max() + 1))
    y_onehot[np.arange(y.size), y] = 1
    return y_onehot


def split_train_test(X, Y, test_size=0.2, random_seed=42):
    """
    Splits the data (X and corresponding labels (y)) into train and test sets according to spesified size
    :param X: The input data
    :param y: The labels
    :param test_size: The size of the test set
    :return: test set and train set: X_train, X_test, y_train, y_test
    """

    X = X.transpose()
    y = Y.transpose()

    np.random.seed(random_seed)
    arr_rand = np.random.rand(X.shape[0])
    split = arr_rand > np.percentile(arr_rand, test_size * 100)

    X_train = X[split].transpose()
    y_train = y[split].transpose()
    X_test = X[~split].transpose()
    y_test = y[~split].transpose()

    return X_train, X_test, y_train, y_test


def norm_data(x):
    """
    Normalize the pixel values of the input data
    :param x: The data to be normalized (array of pixel values from 0-255)
    :return: the normalized data
    """
    x = x / 255
    return x


def load_data():
    """
    Loads the mnist dataset and prepares the data. The labels are transformed from indices to onehot vectors,
    and the matrix shapes are adjusted to the NN dimensions
    :return: mnist train and test sets
    """
    (X_train, y_train), (X_test, y_test) = mnist.load_data()

    X_train = X_train.reshape(X_train.shape[0], -1)
    X_test = X_test.reshape(X_test.shape[0], -1)

    y_train = indice_to_onehot(y_train)
    y_test = indice_to_onehot(y_test)

    X_train, X_test, y_train, y_test = X_train.transpose(), X_test.transpose(), y_train.transpose(), y_test.transpose()

    return X_train, X_test, y_train, y_test

